<div align="center">
  <h1 style="font-size: 2.8em; margin-bottom: 0.4em;">synapz ‚ö°Ô∏è</h1>
  <h3 style="margin-top: 0.4em; margin-bottom: 1em; font-weight: normal;">adaptive learning for neurodiverse students</h3>
  <p style="margin-top: 0.8em;"><i>if learning isn't adapting to how you think, it isn't really teaching you</i></p>
  <p style="margin-top: 0.8em; margin-bottom: 0.8em;">
    <img src="https://img.shields.io/badge/status-research_prototype-blue?style=flat-square" alt="status: research prototype">
    <img src="https://img.shields.io/badge/budget-$50_max-green?style=flat-square" alt="budget: $50 max">
    <img src="https://img.shields.io/badge/evidence-emerging-yellow?style=flat-square" alt="evidence: emerging">
    <img src="https://img.shields.io/badge/license-mit-lightgrey?style=flat-square" alt="license: mit">
  </p>
</div>

## the core question: does adaptive teaching actually work better for neurodiverse students?

synapz is a weekend project trying to answer this. we're specifically looking at adhd, dyslexic, and visual learners. the idea is simple: teaching that molds to your cognitive style should be more effective than a one-size-fits-all approach. but "should be" isn't good enough; we need to measure it.

this project is a sprint, built under tight constraints: a $50 openai api budget and 48 hours on an m4 macbook. these limits forced a lean, focused approach to generating evidence.

## our approach: controlled experiments and careful measurement

to get real answers, we do paired experiments:
*   **adaptive session**: the llm teacher tries to tailor its explanation to a specific learner profile (e.g., using more visuals for a visual learner, or structuring text differently for a dyslexic learner).
*   **control session**: the same llm teacher explains the same concept to the same (simulated) learner profile, but using a generic, non-adapted style.

we use a `teacheragent` to generate explanations and a `studentsimulator` (backed by an llm and heuristics) to provide feedback on clarity and engagement. a `budgettracker` keeps us honest on api costs, and a `metricscalculator` crunches the numbers. for instance, our `batch_run_20250518_121146` processed 33 experiment pairs across different profiles and concepts. everything gets logged to a sqlite database (in wal mode, because we like our data safe).

prompts are externalized in the `prompts/` directory ‚Äì no magic strings in the code.

## how to run it & see what happens

the main engine is `synapz/evaluate.py`.

**to run a new batch of experiments:**
```bash
python -m synapz.evaluate --min-pairs-per-profile 10 --turns 5 --budget 2.0
```
adjust `--min-pairs-per-profile`, `--turns`, and `--budget` as needed.
results (raw csv, compiled json, logs, and charts) land in `results/batch_run_<timestamp>/`.

**to regenerate visualizations from an existing report:**
```bash
python -m synapz.evaluate --create-visuals-for results/your_batch_run_id/compiled_batch_results.json
```
this will create/update a `visualizations` folder next to your report file.

## üìä key visualizations & insights from `batch_run_20250518_121146`

the hard numbers (p-values, specific averages, etc.) for our latest comprehensive run are in `results/batch_run_20250518_121146/compiled_batch_results.json` and the detailed `experiment_pair_details.csv`. we encourage you to dig into these files to see the raw and compiled outputs.

the visualizations in `results/batch_run_20250518_121146/visualizations/` help paint the picture:

<div align="center">
  <p style="margin-bottom: 0.5em;"><strong>overall effectiveness & evidence (`evidence_summary.png`, `readability_metrics.png`):</strong></p>
  <p>
    <img src="./results/batch_run_20250518_121146/visualizations/evidence_summary.png" width="48%" alt="evidence summary chart">
    <img src="./results/batch_run_20250518_121146/visualizations/readability_metrics.png" width="48%" alt="readability & effectiveness metrics chart">
  </p>
  <p style="font-size: 0.9em; margin-top: 0.2em;">
    <em>these give a high-level look at adaptive 'win rates' by profile and overall effectiveness scores. are adaptive methods consistently outperforming control? the answer is likely nuanced and varies by profile.</em>
  </p>
  <hr style="border: none; height: 1px; background-color: #dddddd; margin: 20px 0;">
  <p style="margin-bottom: 0.5em;"><strong>clarity progression over turns (`clarity_progression_adhd.png`, etc.):</strong></p>
  <table role="presentation" style="border-collapse: collapse; width: 100%; margin: 0 auto;">
    <tr>
      <td style="text-align: center; padding: 5px;">
        <img src="./results/batch_run_20250518_121146/visualizations/clarity_progression_adhd.png" width="300" alt="clarity progression for adhd learners">
        <p style="font-size: 0.85em; margin-top: 0;">adhd learners</p>
      </td>
      <td style="text-align: center; padding: 5px;">
        <img src="./results/batch_run_20250518_121146/visualizations/clarity_progression_dyslexic.png" width="300" alt="clarity progression for dyslexic learners">
        <p style="font-size: 0.85em; margin-top: 0;">dyslexic learners</p>
      </td>
      <td style="text-align: center; padding: 5px;">
        <img src="./results/batch_run_20250518_121146/visualizations/clarity_progression_visual.png" width="300" alt="clarity progression for visual learners">
        <p style="font-size: 0.85em; margin-top: 0;">visual learners</p>
      </td>
    </tr>
  </table>
   <p style="font-size: 0.9em; margin-top: 0.2em;">
    <em>these charts track average clarity turn-by-turn. they don't just show *if* learners get it, but *how quickly* and *how consistently*. look for diverging paths between adaptive and control lines ‚Äì sometimes the journey to understanding is more revealing than the destination.</em>
  </p>
   <hr style="border: none; height: 1px; background-color: #dddddd; margin: 20px 0;">
  <p style="margin-bottom: 0.5em;"><strong>magnitude of differences (`effect_sizes.png`):</strong></p>
  <p>
     <img src="./results/batch_run_20250518_121146/visualizations/effect_sizes.png" width="500" alt="effect sizes chart">
  </p>
  <p style="font-size: 0.9em; margin-top: 0.2em;">
    <em>this chart shows cohen's d effect sizes for readability metrics. it tells us *how much* the adaptive explanations differ from control in terms of standard textual properties. significant structural differences (see text/tag diffs in the raw data) might not always translate to large shifts in these standard readability scores, hinting that 'neurodiverse-friendly' might be more complex than just 'simpler text'.</em>
  </p>
</div>

**emerging (and often messy) insights:**

these points are general observations from the project so far, and you can often see them reflected in the detailed data of runs like `batch_run_20250518_121146` by examining the `experiment_pair_details.csv` and the compiled visualizations:

1.  **adaptation is not magic**: simply telling an llm to "be adaptive" for a profile like "visual learner" doesn't automatically yield superior explanations. the *quality and specificity* of adaptation strategies in the prompts are paramount. superficial changes might not cut it. (examine the `experiment_pair_details.csv` for instances where adaptive scores weren't notably higher than control).
2.  **profiles are not monoliths**: what works for one "adhd learner" simulation might not for another, depending on the concept's nature and the specific adaptive tactics tried. the visualizations might show different adaptive "win rates" or clarity trajectories even within the same broad profile category. (look for variance in outcomes for the same profile across different concepts in the batch data).
3.  **simulator fidelity is key (and hard)**: our student simulator (llm + heuristics) tries its best, but it's a proxy. its feedback (clarity, engagement) drives the "measured" success. if the simulator doesn't truly capture a neurodiverse student's interaction patterns, our conclusions are built on a shaky foundation. the heuristic metrics provide a partial cross-check. (consider how different simulated feedback patterns in the raw data contribute to overall scores).
4.  **"clarity" is multifaceted**: an llm-simulated clarity score is one thing. objective readability scores (like flesch-kincaid from `textstat`) are another. the `effect_sizes.png` might show that adaptive explanations are, say, *more* complex by one metric but perceived as clearer by the simulator. this tension, visible when comparing `readability_metrics.png` and `clarity_progression` charts for `batch_run_20250518_121146`, is where interesting insights lie.
5.  **statistical significance vs. practical impact**: with enough runs (like the 33 pairs in `batch_run_20250518_121146`), small differences can become statistically significant (low p-values). the `effect_sizes.png` (cohen's d) helps us see if these differences are also practically meaningful. sometimes, an adaptation might be "better" but not by much.

the journey is iterative, and each batch run (like `batch_run_20250518_121146`) is a cycle:
1.  **analyze results**: we dig into the data (`experiment_pair_details.csv`, `compiled_batch_results.json`) and visualizations from the latest run.
2.  **refine hypotheses**: based on what worked (or didn't), we update the teaching strategies in `prompts/` and consider tweaks to the `studentsimulator`'s logic or heuristic metrics.
3.  **test again**: new experiments are run with `evaluate.py` to measure the impact of our refinements.
this learn-adjust-retest loop is how synapz evolves and deepens its understanding of "effective adaptation."

## the tricky bits & what's next

this is research, so it's full of challenges, many of which were apparent during the execution and analysis of `batch_run_20250518_121146`:
*   **getting enough data**: small sample sizes (even with ~10-11 pairs per profile as in our latest run) make it hard to draw strong statistical conclusions for specific profiles vs. specific concepts. we need more runs to confirm trends.
*   **defining "good" adaptation**: what specific instructional strategies truly work best for an adhd profile when explaining algebra vs. history? our prompts are hypotheses themselves, and their performance in `batch_run_20250518_121146` gives clues for refinement.
*   **the cost constraint**: the $50 budget (though extended for the larger run) means we can't just run thousands of experiments with the most powerful models. every llm call counts, a factor carefully managed in all runs.

**immediate focus:**
1.  **more data, wisely**: run more experiment pairs, perhaps focusing on profiles/concepts where results from `batch_run_20250518_121146` were ambiguous or counterintuitive. use the `--min-pairs-per-profile` flag in `evaluate.py`.
2.  **prompt refinement**: based on the detailed data in `experiment_pair_details.csv` from `batch_run_20250518_121146`, identify adaptive sessions that underperformed and analyze *why*. update the instruction sets in `prompts/`.
3.  **simulator enhancements**: can we add more sophisticated heuristics to `studentsimulator` that better correlate with neurodiverse processing, informed by patterns seen in the latest run data? for example, tracking cognitive load indicators beyond simple readability.

## üèóÔ∏è project structure

```
synapz/
‚îú‚îÄ‚îÄ core/               # teacheragent, studentsimulator, budget, llmclient, db models
‚îú‚îÄ‚îÄ data/               # concept .json files, profile .json files, metrics.py, visualization.py
‚îú‚îÄ‚îÄ prompts/            # .txt files for system & instruction prompts
‚îú‚îÄ‚îÄ results/            # timestamped output from batch runs
‚îú‚îÄ‚îÄ tests/              # (aspiring to have more of these)
‚îú‚îÄ‚îÄ evaluate.py         # main batch evaluation script
‚îî‚îÄ‚îÄ cli.py              # (currently minimal, for potential interactive testing)
```

## üì¶ installation

```bash
# clone
git clone https://github.com/dipampaul17/synapz.git
cd synapz

# env
python3 -m venv .venv
source .venv/bin/activate  # on windows: .venv\\scripts\\activate

# install
pip install -r requirements.txt

# api key
export openai_api_key='your-api-key' # or pass via --api-key in evaluate.py
```

## üè∑Ô∏è tags

`adaptive-learning` `cognitive-diversity` `llm-education` `neurodiversity` `adhd` `dyslexia` `visual-learner` `personalized-learning` `prompt-engineering` `educational-technology` `learning-science` `experiment-design` `python` `openai-api` `evidence-based-education`

## üìë license

mit

---

<div align="center">
  <p>this is a research sprint. the goal is learning, iterating, and (hopefully) finding some truth.</p>
  <p><a href="https://github.com/dipampaul17/synapz">github.com/dipampaul17/synapz</a></p>
</div> 